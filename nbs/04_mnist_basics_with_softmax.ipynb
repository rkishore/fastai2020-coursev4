{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring loss functions for the MNIST_SAMPLE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** In this notebook, I want to move towards classifying the full MNIST dataset of ten digits by first working with different loss functions for the smaller MNIST_SAMPLE dataset. The MNIST_SAMPLE dataset has data only for two digits (3s and 7s). The loss functions I explore are the `mnist_loss`, `softmax_loss`, and `cross_entropy_loss`. The reference for everything in this blog post is the [fastai 2020 course](https://course.fast.ai), especially the [amazing textbook](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527).\n",
    "\n",
    "In Chapter 4 of the textbook, we are taught how to get the data ready and to use the `mnist_loss` function that basically uses the `sigmoid` function on one column of activations from the final layer. Then, in Chapter 5, we are given examples of how to use the `softmax` function to achieve what the `sigmoid` function does, but on more than one column of activations. Moreover, the `cross_entropy_loss` function is introduced. This function basically adds a `log` to the `softmax` function, i.e. it does a `log_softmax` on the final layer of activations followed by selecting the loss corresponding to the column that corresponds to the target (using `nll_loss`). \n",
    "\n",
    "In this notebook, I want to learn how to use a loss function that will work with more than one column of activations from the final layer. With the MNIST_FULL dataset, my thinking is that it would be better to have ten columns of activations from the final layer as opposed to one really long column of activations. And beyond this, as per Chatper 5 in the textbook, we should use the `cross_entropy_loss` to aid classification among ten categories of images. Here, I want to understand the details of this planned approach and validate it with two digits before moving to classify data from ten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the basic imports out of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/home/igolgi/.fastai/data/mnist_sample/valid'),Path('/home/igolgi/.fastai/data/mnist_sample/labels.csv'),Path('/home/igolgi/.fastai/data/mnist_sample/train'),Path('/home/igolgi/.fastai/data/mnist_sample/models')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_imgs = (path/'train'/'3').ls().sorted()\n",
    "seven_imgs = (path/'train'/'7').ls().sorted()\n",
    "\n",
    "three_tensors_list = [tensor(Image.open(img)) for img in three_imgs]\n",
    "seven_tensors_list = [tensor(Image.open(img)) for img in seven_imgs]\n",
    "\n",
    "stacked_threes = torch.stack(three_tensors_list).float()/255.\n",
    "stacked_sevens = torch.stack(seven_tensors_list).float()/255.\n",
    "stacked_threes.shape, stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(three_tensors_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(seven_tensors_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]); train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.view(-1, 28*28); train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor( [1]*len(three_imgs) + [0]*len(seven_imgs) ).unsqueeze(1); train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_three_imgs = (path/'train'/'3').ls().sorted()\n",
    "valid_seven_imgs = (path/'train'/'7').ls().sorted()\n",
    "\n",
    "valid_three_tensors_list = [tensor(Image.open(img)) for img in valid_three_imgs]\n",
    "valid_seven_tensors_list = [tensor(Image.open(img)) for img in valid_seven_imgs]\n",
    "\n",
    "valid_stacked_threes = torch.stack(valid_three_tensors_list).float()/255.\n",
    "valid_stacked_sevens = torch.stack(valid_seven_tensors_list).float()/255.\n",
    "valid_stacked_threes.shape, valid_stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = torch.cat([valid_stacked_threes, valid_stacked_sevens]).view(-1, 28*28); valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y = tensor( [1]*len(valid_three_imgs) + [0]*len(valid_seven_imgs) ).unsqueeze(1); valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = list(zip(train_x, train_y))\n",
    "valid_dset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a basic linear classifier going as well as the loss functions, optimizers and metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb, weights, bias): return xb@weights + bias # linear classifier with one weight per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(preds, tgts):\n",
    "    preds = preds.sigmoid()\n",
    "    return torch.where(tgts==1, 1-preds, preds).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    log_preds = torch.log(preds)\n",
    "    #print(preds.shape, preds[0], preds[:,0].shape[0])\n",
    "    return F.nll_loss(log_preds, torch.squeeze(tgts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    idx = range(preds[:,0].shape[0])\n",
    "    return preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_mnist_loss(xb, yb, model, params):\n",
    "    preds = model(xb, params[0], params[1])\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_ce_loss(xb, yb, model, params):\n",
    "    preds = model(xb, params[0], params[1])\n",
    "    loss = cross_entropy_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_softmax_loss(xb, yb, model, params):\n",
    "    preds = model(xb, params[0], params[1])\n",
    "    loss = softmax_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_weights(params, lr):\n",
    "    for p in params:\n",
    "        p.data -= p.grad*lr\n",
    "        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params, calc_grad_func):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad_func(xb,yb,model,params)\n",
    "        step_weights(params,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb,yb, mnist=True):\n",
    "    if mnist:\n",
    "        preds = xb.sigmoid()\n",
    "        correct = (preds>0.5) == yb\n",
    "    else:\n",
    "        preds = torch.softmax(xb, dim=1)\n",
    "        yb_squeezed = torch.squeeze(yb)\n",
    "        #print(xb.shape, yb.shape, preds.shape, yb_squeezed.shape)\n",
    "        correct = (preds[:,0]>0.5) == yb_squeezed\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, params, mnist=True):\n",
    "    accs = [batch_accuracy(model(xb, params[0], params[1]), yb, mnist) for xb,yb in valid_dl]\n",
    "    if mnist:\n",
    "        return round(torch.stack(accs).mean().item(), 4)\n",
    "    else:\n",
    "        accs_tensor = tensor(accs)\n",
    "        #print(len(accs), accs[0], accs_tensor[0], accs_tensor.mean())\n",
    "        accuracy = round(accs_tensor.mean().item(), 4)\n",
    "        #print(\"Accuracy: \", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets use mnist_loss and one column of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7205 0.8401 0.8998 0.9242 0.9381 0.9464 0.9534 0.9573 0.9608 0.9628 "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "lr = 1.0\n",
    "params = [weights, bias]\n",
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params, calc_grad_mnist_loss)\n",
    "    print(validate_epoch(linear1, params), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use cross_entropy_loss and two columns of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n",
      "0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28,2))\n",
    "bias = init_params(2)\n",
    "lr = 1.0\n",
    "params = [weights, bias]\n",
    "print(weights.shape, bias.shape)\n",
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params, calc_grad_ce_loss)\n",
    "    print(validate_epoch(linear1, params, False), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the `batch_accuracy` function is not increasing at all. We need to debug what is going on here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug why the batch accuracy is not changing when we use cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = init_params((28*28,2)); b = init_params(2); w.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(x, w, b); preds[:,0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7301,  4.6855], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0518, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = cross_entropy_loss(preds, y); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = w,b\n",
    "for p in params:\n",
    "    #print(p.grad, p.grad.mean())\n",
    "    p.data -= p.grad*lr\n",
    "    p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2188)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(preds, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "w = init_params((28*28,2)); b = init_params(2); print(w.shape, b.shape)\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    #print(preds.shape, preds[0], preds[:,0].shape[0])\n",
    "    idx = range(preds[:,0].shape[0])\n",
    "    return preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    log_preds = torch.log(preds)\n",
    "    #print(preds.shape, preds[0], preds[:,0].shape[0])\n",
    "    #return F.nll_loss(log_preds, torch.squeeze(tgts)) # This is the culprit!\n",
    "    idx = range(log_preds[:,0].shape[0])\n",
    "    return log_preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_single(x1, y1, w1, b1):\n",
    "    preds = linear1(x1, w1, b1);\n",
    "    loss = softmax_loss(preds, y1); \n",
    "    #print(\"Loss: \", loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_weights_single(p):\n",
    "    p_grad_mean = p.grad.mean()\n",
    "    p.data -= p.grad*lr\n",
    "    p.grad.zero_()\n",
    "    return p_grad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_single():\n",
    "    max_batches = 5\n",
    "    cur_batch = 0\n",
    "    batch_loss = []\n",
    "    for xb, yb in dl:\n",
    "        batch_loss.append(calc_grad_single(xb, yb, w, b))\n",
    "        w_grad_mean = step_weights_single(w)\n",
    "        b_grad_mean = step_weights_single(b)\n",
    "        #print(\"Weights.grad.mean and bias.grad.mean: \", w_grad_mean, b_grad_mean)\n",
    "        cur_batch += 1\n",
    "        #if cur_batch >= max_batches:\n",
    "        #    break\n",
    "    batch_loss_tensor = tensor(batch_loss)\n",
    "    print(\"Mean loss: \", batch_loss_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy_single(xb, yb):\n",
    "    preds = torch.softmax(xb, dim=1)\n",
    "    yb_squeezed = torch.squeeze(yb)\n",
    "    #print(xb.shape, yb.shape, preds.shape, yb_squeezed.shape)\n",
    "    correct = (preds[:,0]>0.5) == yb_squeezed\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_single(model):\n",
    "    accs = [batch_accuracy_single(model(xb,w,b), yb) for xb,yb in valid_dl]\n",
    "    accs_tensor = tensor(accs)\n",
    "    #print(len(accs), accs[0], accs_tensor[0], accs_tensor.mean())\n",
    "    accuracy = round(accs_tensor.mean().item(), 4)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n",
      "Mean loss:  tensor(0.6547)\n",
      "Accuracy:  0.425\n",
      "Mean loss:  tensor(0.5197)\n",
      "Accuracy:  0.5398\n",
      "Mean loss:  tensor(0.4005)\n",
      "Accuracy:  0.6602\n",
      "Mean loss:  tensor(0.2695)\n",
      "Accuracy:  0.7681\n",
      "Mean loss:  tensor(0.1824)\n",
      "Accuracy:  0.8331\n",
      "Mean loss:  tensor(0.1432)\n",
      "Accuracy:  0.8711\n",
      "Mean loss:  tensor(0.1168)\n",
      "Accuracy:  0.8934\n",
      "Mean loss:  tensor(0.1001)\n",
      "Accuracy:  0.9078\n",
      "Mean loss:  tensor(0.0886)\n",
      "Accuracy:  0.9191\n",
      "Mean loss:  tensor(0.0804)\n",
      "Accuracy:  0.9267\n"
     ]
    }
   ],
   "source": [
    "w = init_params((28*28,2)); b = init_params(2); print(w.shape, b.shape)\n",
    "lr = 0.1\n",
    "for i in range(10):\n",
    "    train_epoch_single()\n",
    "    validate_epoch_single(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** when we use softmax as a loss function, the behavior is as expected, i.e. the loss goes down and the `batch_accuracy` goes up. But with `cross_entropy_loss`, the `batch_accuracy` stays constant. Why? Needs to be investigated further. For now, we plan to use `softmax_loss` till we figure out why going forward with MNIST_FULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n",
      "0.6858 0.8494 0.913 0.9315 0.9449 0.9523 0.9563 0.9603 0.9619 0.9642 "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28,2))\n",
    "bias = init_params(2)\n",
    "lr = 1.0\n",
    "params = [weights, bias]\n",
    "print(weights.shape, bias.shape)\n",
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params, calc_grad_softmax_loss)\n",
    "    print(validate_epoch(linear1, params, False), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I could not get the traditional `cross_entropy_loss` using the negative log-likelihood function to work as expected with the MNIST_SAMPLE dataset of '3's and '7's and two columns of activations from the final layer. This is, in all likelihood, due to my inability to define the `batch_accuracy` function correctly? Using the `softmax_loss` alone on the other hand works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's replace our code with PyTorch/fastai built-in functions and see if we get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 784]), torch.Size([2]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear2 = nn.Linear(28*28, 2)\n",
    "w,b = linear2.parameters(); w.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put our step_weights function into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear2.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets redefine our loss functions to match the template expected by fastai and PyTorch. Lets also redefine the `train_epoch` function to use the optimizer defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    #print(preds[0])\n",
    "    idx = range(preds[:,0].shape[0])\n",
    "    return preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_mnist_loss(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_ce_loss(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = cross_entropy_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_softmax_loss(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = softmax_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, calc_grad_func):\n",
    "    epoch_loss = []\n",
    "    for xb,yb in dl:\n",
    "        epoch_loss.append(calc_grad_func(xb,yb,model))\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    epoch_loss_tnsr = tensor(epoch_loss)\n",
    "    return epoch_loss_tnsr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to redefine the validate_epoch function simply because we don't need to pass the params as args to the built-in Pytorch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_new(model, mnist=True):\n",
    "    accs = [batch_accuracy(model(xb), yb, mnist) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6437"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch_new(linear2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(linear2, calc_grad_softmax_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, mnist=True):\n",
    "    for i in range(epochs):\n",
    "        if mnist:\n",
    "            epoch_loss_mean = train_epoch(model, calc_grad_mnist_loss)\n",
    "        else:\n",
    "            epoch_loss_mean = train_epoch(model, calc_grad_softmax_loss)\n",
    "        #print(epoch_loss_mean)\n",
    "        print(validate_epoch_new(model, mnist), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4888 0.8616 0.8242 0.9001 0.9282 0.944 0.9525 0.9581 0.9636 0.9676 "
     ]
    }
   ],
   "source": [
    "# mnist_loss\n",
    "lr = 1.\n",
    "linear2 = nn.Linear(28*28,1)\n",
    "#opt = SGD(linear2.parameters(), lr)\n",
    "opt = BasicOptim(linear2.parameters(), lr)\n",
    "train_model(linear2, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5217 0.8304 0.9157 0.9427 0.9552 0.9628 0.9674 0.9707 0.9723 0.9737 "
     ]
    }
   ],
   "source": [
    "# softmax_loss\n",
    "lr = 0.1\n",
    "linear3 = nn.Linear(28*28,2)\n",
    "#opt = SGD(linear3.parameters(), lr)\n",
    "opt = BasicOptim(linear3.parameters(), lr)\n",
    "train_model(linear3, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple replacement we can make for our BasicOptim optimizer class is to replace it with the built-in SGD optimizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4888 0.7525 0.8579 0.9094 0.9333 0.9467 0.9546 0.9595 0.9648 0.9688 "
     ]
    }
   ],
   "source": [
    "# mnist_loss\n",
    "lr = 1.\n",
    "linear2 = nn.Linear(28*28,1)\n",
    "opt = SGD(linear2.parameters(), lr)\n",
    "train_model(linear2, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5363 0.8386 0.9193 0.9441 0.955 0.9636 0.9671 0.9698 0.9718 0.9733 "
     ]
    }
   ],
   "source": [
    "# softmax_loss\n",
    "lr = 0.1\n",
    "linear3 = nn.Linear(28*28,2)\n",
    "opt = SGD(linear3.parameters(), lr)\n",
    "train_model(linear3, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `train_model`, we can now transition over to using the built-in `Learner.fit` class method. Before we do this, lets redefine batch_accuracy for mnist and softmax separately as there is a need to follow the template here for this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy_mnist(xb,yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy_softmax(xb,yb):\n",
    "    preds = torch.softmax(xb, dim=1)\n",
    "    yb_squeezed = torch.squeeze(yb)\n",
    "    #print(xb.shape, yb.shape, preds.shape, yb_squeezed.shape)\n",
    "    correct = (preds[:,0]>0.5) == yb_squeezed\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_mnist = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_softmax = Learner(dls, nn.Linear(28*28,2), opt_func=SGD, loss_func=softmax_loss, metrics=batch_accuracy_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_mnist</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.637177</td>\n",
       "      <td>0.504163</td>\n",
       "      <td>0.494595</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.506652</td>\n",
       "      <td>0.218602</td>\n",
       "      <td>0.804453</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.187165</td>\n",
       "      <td>0.177797</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082407</td>\n",
       "      <td>0.109837</td>\n",
       "      <td>0.902065</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043772</td>\n",
       "      <td>0.081640</td>\n",
       "      <td>0.928929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.028667</td>\n",
       "      <td>0.065939</td>\n",
       "      <td>0.944256</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.055925</td>\n",
       "      <td>0.952323</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.048953</td>\n",
       "      <td>0.958454</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.043798</td>\n",
       "      <td>0.963779</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.039850</td>\n",
       "      <td>0.967570</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_mnist.fit(10, lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_softmax</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.393483</td>\n",
       "      <td>0.524685</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.089713</td>\n",
       "      <td>0.191804</td>\n",
       "      <td>0.841884</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.060771</td>\n",
       "      <td>0.111265</td>\n",
       "      <td>0.918361</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047979</td>\n",
       "      <td>0.080447</td>\n",
       "      <td>0.942965</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.041427</td>\n",
       "      <td>0.064905</td>\n",
       "      <td>0.954743</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.055631</td>\n",
       "      <td>0.962085</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035008</td>\n",
       "      <td>0.049473</td>\n",
       "      <td>0.966602</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>0.970152</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>0.041761</td>\n",
       "      <td>0.971442</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030383</td>\n",
       "      <td>0.039169</td>\n",
       "      <td>0.973459</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_softmax.fit(10, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a neural network instead of a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net_mnist = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1) # 1 column of activations from the final layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net_softmax = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,2) # 2 columns of activations from the final layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_mnist = Learner(dls, simple_net_mnist, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_softmax = Learner(dls, simple_net_softmax, opt_func=SGD, loss_func=softmax_loss, metrics=batch_accuracy_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_mnist</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.298082</td>\n",
       "      <td>0.406346</td>\n",
       "      <td>0.506050</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.140334</td>\n",
       "      <td>0.229533</td>\n",
       "      <td>0.802194</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078882</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.906986</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052542</td>\n",
       "      <td>0.082606</td>\n",
       "      <td>0.937964</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040197</td>\n",
       "      <td>0.064589</td>\n",
       "      <td>0.950307</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033815</td>\n",
       "      <td>0.054133</td>\n",
       "      <td>0.959180</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>0.047274</td>\n",
       "      <td>0.964343</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.042405</td>\n",
       "      <td>0.967328</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>0.038752</td>\n",
       "      <td>0.969990</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024566</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.972088</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023452</td>\n",
       "      <td>0.033599</td>\n",
       "      <td>0.973540</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022520</td>\n",
       "      <td>0.031709</td>\n",
       "      <td>0.974992</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021722</td>\n",
       "      <td>0.030128</td>\n",
       "      <td>0.976283</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021032</td>\n",
       "      <td>0.028784</td>\n",
       "      <td>0.977251</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.027628</td>\n",
       "      <td>0.977977</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019889</td>\n",
       "      <td>0.026624</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019410</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.024963</td>\n",
       "      <td>0.979348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018585</td>\n",
       "      <td>0.024268</td>\n",
       "      <td>0.979671</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018227</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>0.980155</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_mnist.fit(20, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_softmax</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.248439</td>\n",
       "      <td>0.425511</td>\n",
       "      <td>0.505566</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115231</td>\n",
       "      <td>0.213843</td>\n",
       "      <td>0.813892</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>0.113364</td>\n",
       "      <td>0.907632</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.936189</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032682</td>\n",
       "      <td>0.062401</td>\n",
       "      <td>0.950065</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.052443</td>\n",
       "      <td>0.957567</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024883</td>\n",
       "      <td>0.045787</td>\n",
       "      <td>0.963053</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.040991</td>\n",
       "      <td>0.966925</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.037365</td>\n",
       "      <td>0.969426</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>0.034523</td>\n",
       "      <td>0.971846</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019718</td>\n",
       "      <td>0.032235</td>\n",
       "      <td>0.973701</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.018994</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>0.975073</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.028790</td>\n",
       "      <td>0.976444</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.017839</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>0.977251</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.017366</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>0.977735</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016946</td>\n",
       "      <td>0.025345</td>\n",
       "      <td>0.978219</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>0.978783</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016228</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.979590</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.980236</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015633</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>0.980800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_softmax.fit(20, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty cool accuracy numbers there! :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
