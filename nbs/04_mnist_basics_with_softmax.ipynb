{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy loss and the MNIST_SAMPLE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** In this notebook, I want to move towards classifying the full MNIST dataset by first using cross entropy loss or softmax as a loss function for the MNIST_SAMPLE dataset that has data only for two digits (3s and 7s), as opposed to the full ten digits in the MNIST dataset.\n",
    "\n",
    "In Chapter 4 of the textbook, we are taught how to get the data ready and to use the mnist_loss function that basically uses the `sigmoid` function on one column of activations from the final layer. Then, in Chapter 5, we are given examples of how to use the `softmax` function to achieve what the `sigmoid` function does, but on more than one column of activations. Moreover, the cross entropy loss is introduced that basically does a `log_softmax` on the final layer of activations followed by selecting the loss corresponding to the column that corresponds to the target of interest (using `nll_loss`).\n",
    "\n",
    "With the MNIST_FULL dataset, my thinking is that it would be better to have ten columns of activations from the final layer as opposed to one really long column of activations. And beyond this, we can use the cross entropy loss to aid classification among ten categories. Here, I want to understand the details of this planned approach and validate it with two digits before moving to classify data from ten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/home/igolgi/.fastai/data/mnist_sample/valid'),Path('/home/igolgi/.fastai/data/mnist_sample/labels.csv'),Path('/home/igolgi/.fastai/data/mnist_sample/train'),Path('/home/igolgi/.fastai/data/mnist_sample/models')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_imgs = (path/'train'/'3').ls().sorted()\n",
    "seven_imgs = (path/'train'/'7').ls().sorted()\n",
    "\n",
    "three_tensors_list = [tensor(Image.open(img)) for img in three_imgs]\n",
    "seven_tensors_list = [tensor(Image.open(img)) for img in seven_imgs]\n",
    "\n",
    "stacked_threes = torch.stack(three_tensors_list).float()/255.\n",
    "stacked_sevens = torch.stack(seven_tensors_list).float()/255.\n",
    "stacked_threes.shape, stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]); train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.view(-1, 28*28); train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor( [1]*len(three_imgs) + [0]*len(seven_imgs) ).unsqueeze(1); train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_three_imgs = (path/'train'/'3').ls().sorted()\n",
    "valid_seven_imgs = (path/'train'/'7').ls().sorted()\n",
    "\n",
    "valid_three_tensors_list = [tensor(Image.open(img)) for img in valid_three_imgs]\n",
    "valid_seven_tensors_list = [tensor(Image.open(img)) for img in valid_seven_imgs]\n",
    "\n",
    "valid_stacked_threes = torch.stack(valid_three_tensors_list).float()/255.\n",
    "valid_stacked_sevens = torch.stack(valid_seven_tensors_list).float()/255.\n",
    "valid_stacked_threes.shape, valid_stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = torch.cat([valid_stacked_threes, valid_stacked_sevens]).view(-1, 28*28); valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y = tensor( [1]*len(valid_three_imgs) + [0]*len(valid_seven_imgs) ).unsqueeze(1); valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = list(zip(train_x, train_y))\n",
    "valid_dset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a basic linear classifier going as well as the loss functions, optimizers and metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb, weights, bias): return xb@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(preds, tgts):\n",
    "    preds = preds.sigmoid()\n",
    "    return torch.where(tgts==1, 1-preds, preds).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    log_preds = torch.log(preds)\n",
    "    #print(preds.shape, preds[0], preds[:,0].shape[0])\n",
    "    return F.nll_loss(log_preds, torch.squeeze(tgts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define and use the softmax function as our loss as using the cross entropy function results in weird behavior\n",
    "# We need to get to the bottom of this weird behavior\n",
    "def softmax_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    idx = range(preds[:,0].shape[0])\n",
    "    return preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_mnist_loss(xb, yb, model, params):\n",
    "    preds = model(xb, params[0], params[1])\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_ce_loss(xb, yb, model, params):\n",
    "    preds = model(xb, params[0], params[1])\n",
    "    loss = cross_entropy_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_softmax_loss(xb, yb, model, params):\n",
    "    preds = model(xb, params[0], params[1])\n",
    "    loss = softmax_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_weights(params, lr):\n",
    "    for p in params:\n",
    "        p.data -= p.grad*lr\n",
    "        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params, calc_grad_func):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad_func(xb,yb,model,params)\n",
    "        step_weights(params,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb,yb, mnist=True):\n",
    "    if mnist:\n",
    "        preds = xb.sigmoid()\n",
    "        correct = (preds>0.5) == yb\n",
    "    else:\n",
    "        preds = torch.softmax(xb, dim=1)\n",
    "        yb_squeezed = torch.squeeze(yb)\n",
    "        #print(xb.shape, yb.shape, preds.shape, yb_squeezed.shape)\n",
    "        correct = (preds[:,0]>0.5) == yb_squeezed\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, params, mnist=True):\n",
    "    accs = [batch_accuracy(model(xb, params[0], params[1]), yb, mnist) for xb,yb in valid_dl]\n",
    "    if mnist:\n",
    "        return round(torch.stack(accs).mean().item(), 4)\n",
    "    else:\n",
    "        accs_tensor = tensor(accs)\n",
    "        #print(len(accs), accs[0], accs_tensor[0], accs_tensor.mean())\n",
    "        accuracy = round(accs_tensor.mean().item(), 4)\n",
    "        #print(\"Accuracy: \", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets use mnist_loss and one column of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7205 0.8401 0.8998 0.9242 0.9381 0.9464 0.9534 0.9573 0.9608 0.9628 "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "lr = 1.0\n",
    "params = [weights, bias]\n",
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params, calc_grad_mnist_loss)\n",
    "    print(validate_epoch(linear1, params), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use cross_entropy_loss and two columns of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n",
      "0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 0.5112 "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28,2))\n",
    "bias = init_params(2)\n",
    "lr = 1.0\n",
    "params = [weights, bias]\n",
    "print(weights.shape, bias.shape)\n",
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params, calc_grad_ce_loss)\n",
    "    print(validate_epoch(linear1, params, False), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug why the batch accuracy is not changing when we use cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = init_params((28*28,2)); b = init_params(2); w.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(x, w, b); preds[:,0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7301,  4.6855], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0518, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = cross_entropy_loss(preds, y); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = w,b\n",
    "for p in params:\n",
    "    #print(p.grad, p.grad.mean())\n",
    "    p.data -= p.grad*lr\n",
    "    p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2188)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(preds, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "w = init_params((28*28,2)); b = init_params(2); print(w.shape, b.shape)\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    #print(preds.shape, preds[0], preds[:,0].shape[0])\n",
    "    idx = range(preds[:,0].shape[0])\n",
    "    return preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    log_preds = torch.log(preds)\n",
    "    #print(preds.shape, preds[0], preds[:,0].shape[0])\n",
    "    #return F.nll_loss(log_preds, torch.squeeze(tgts)) # This is the culprit!\n",
    "    idx = range(log_preds[:,0].shape[0])\n",
    "    return log_preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_single(x1, y1, w1, b1):\n",
    "    preds = linear1(x1, w1, b1);\n",
    "    loss = softmax_loss(preds, y1); \n",
    "    #print(\"Loss: \", loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_weights_single(p):\n",
    "    p_grad_mean = p.grad.mean()\n",
    "    p.data -= p.grad*lr\n",
    "    p.grad.zero_()\n",
    "    return p_grad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_single():\n",
    "    max_batches = 5\n",
    "    cur_batch = 0\n",
    "    batch_loss = []\n",
    "    for xb, yb in dl:\n",
    "        batch_loss.append(calc_grad_single(xb, yb, w, b))\n",
    "        w_grad_mean = step_weights_single(w)\n",
    "        b_grad_mean = step_weights_single(b)\n",
    "        #print(\"Weights.grad.mean and bias.grad.mean: \", w_grad_mean, b_grad_mean)\n",
    "        cur_batch += 1\n",
    "        #if cur_batch >= max_batches:\n",
    "        #    break\n",
    "    batch_loss_tensor = tensor(batch_loss)\n",
    "    print(\"Mean loss: \", batch_loss_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy_single(xb, yb):\n",
    "    preds = torch.softmax(xb, dim=1)\n",
    "    yb_squeezed = torch.squeeze(yb)\n",
    "    #print(xb.shape, yb.shape, preds.shape, yb_squeezed.shape)\n",
    "    correct = (preds[:,0]>0.5) == yb_squeezed\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_single(model):\n",
    "    accs = [batch_accuracy_single(model(xb,w,b), yb) for xb,yb in valid_dl]\n",
    "    accs_tensor = tensor(accs)\n",
    "    #print(len(accs), accs[0], accs_tensor[0], accs_tensor.mean())\n",
    "    accuracy = round(accs_tensor.mean().item(), 4)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n",
      "Mean loss:  tensor(0.6547)\n",
      "Accuracy:  0.425\n",
      "Mean loss:  tensor(0.5197)\n",
      "Accuracy:  0.5398\n",
      "Mean loss:  tensor(0.4005)\n",
      "Accuracy:  0.6602\n",
      "Mean loss:  tensor(0.2695)\n",
      "Accuracy:  0.7681\n",
      "Mean loss:  tensor(0.1824)\n",
      "Accuracy:  0.8331\n",
      "Mean loss:  tensor(0.1432)\n",
      "Accuracy:  0.8711\n",
      "Mean loss:  tensor(0.1168)\n",
      "Accuracy:  0.8934\n",
      "Mean loss:  tensor(0.1001)\n",
      "Accuracy:  0.9078\n",
      "Mean loss:  tensor(0.0886)\n",
      "Accuracy:  0.9191\n",
      "Mean loss:  tensor(0.0804)\n",
      "Accuracy:  0.9267\n"
     ]
    }
   ],
   "source": [
    "w = init_params((28*28,2)); b = init_params(2); print(w.shape, b.shape)\n",
    "lr = 0.1\n",
    "for i in range(10):\n",
    "    train_epoch_single()\n",
    "    validate_epoch_single(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2]) torch.Size([2])\n",
      "0.6858 0.8494 0.913 0.9315 0.9449 0.9523 0.9563 0.9603 0.9619 0.9642 "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28,2))\n",
    "bias = init_params(2)\n",
    "lr = 1.0\n",
    "params = [weights, bias]\n",
    "print(weights.shape, bias.shape)\n",
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params, calc_grad_softmax_loss)\n",
    "    print(validate_epoch(linear1, params, False), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I could not get the traditional cross-entropy loss using the negative log-likelihood function to work well with the MNIST_SAMPLE dataset of '3's and '7's. Using softmax alone, on the other hand works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's replace our code with PyTorch/fastai built-in functions and see if we get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 784]), torch.Size([2]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear2 = nn.Linear(28*28, 2)\n",
    "w,b = linear2.parameters(); w.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put our step_weights function into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear2.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets redefine our train_epoch function to use the optimizer object above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(preds, tgts):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    #print(preds[0])\n",
    "    idx = range(preds[:,0].shape[0])\n",
    "    return preds[idx, torch.squeeze(tgts)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_mnist_loss(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_ce_loss(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = cross_entropy_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_softmax_loss(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = softmax_loss(preds, yb)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, calc_grad_func):\n",
    "    epoch_loss = []\n",
    "    for xb,yb in dl:\n",
    "        epoch_loss.append(calc_grad_func(xb,yb,model))\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    epoch_loss_tnsr = tensor(epoch_loss)\n",
    "    return epoch_loss_tnsr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to redefine the validate_epoch function simply because we don't need to pass the params as args to the built-in Pytorch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_new(model, mnist=True):\n",
    "    accs = [batch_accuracy(model(xb), yb, mnist) for xb,yb in valid_dl]\n",
    "    #if mnist:\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "    #else:\n",
    "    #    accs_tensor = tensor(accs)\n",
    "        #print(len(accs), accs[0], accs_tensor[0], accs_tensor.mean())\n",
    "    #    accuracy = round(accs_tensor.mean().item(), 4)\n",
    "        #print(\"Accuracy: \", accuracy)\n",
    "    #    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6437"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch_new(linear2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(linear2, calc_grad_softmax_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, mnist=True):\n",
    "    for i in range(epochs):\n",
    "        if mnist:\n",
    "            epoch_loss_mean = train_epoch(model, calc_grad_mnist_loss)\n",
    "        else:\n",
    "            epoch_loss_mean = train_epoch(model, calc_grad_softmax_loss)\n",
    "        #print(epoch_loss_mean)\n",
    "        print(validate_epoch_new(model, mnist), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4888 0.7474 0.8562 0.9078 0.9326 0.9458 0.9539 0.9594 0.9644 0.9685 "
     ]
    }
   ],
   "source": [
    "# mnist_loss\n",
    "lr = 1.\n",
    "linear2 = nn.Linear(28*28,1)\n",
    "#opt = SGD(linear2.parameters(), lr)\n",
    "opt = BasicOptim(linear2.parameters(), lr)\n",
    "train_model(linear2, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418 0.8451 0.9199 0.944 0.9561 0.963 0.9669 0.9696 0.9723 0.9741 "
     ]
    }
   ],
   "source": [
    "# softmax_loss\n",
    "lr = 0.1\n",
    "linear3 = nn.Linear(28*28,2)\n",
    "#opt = SGD(linear3.parameters(), lr)\n",
    "opt = BasicOptim(linear3.parameters(), lr)\n",
    "train_model(linear3, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple replacement we can make for our BasicOptim optimizer class is to replace it with the built-in SGD optimizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4888 0.7524 0.8551 0.9088 0.933 0.9463 0.9541 0.959 0.9645 0.9684 "
     ]
    }
   ],
   "source": [
    "# mnist_loss\n",
    "lr = 1.\n",
    "linear2 = nn.Linear(28*28,1)\n",
    "opt = SGD(linear2.parameters(), lr)\n",
    "train_model(linear2, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5283 0.8414 0.92 0.9442 0.9561 0.9633 0.9673 0.9705 0.9724 0.9738 "
     ]
    }
   ],
   "source": [
    "# softmax_loss\n",
    "lr = 0.1\n",
    "linear3 = nn.Linear(28*28,2)\n",
    "opt = SGD(linear3.parameters(), lr)\n",
    "train_model(linear3, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `train_model`, we can now transition over to using the built-in `Learner.fit` class method. Before we do this, lets redefine batch_accuracy for mnist and softmax separately as there is a need to follow the template here for this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy_mnist(xb,yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy_softmax(xb,yb):\n",
    "    preds = torch.softmax(xb, dim=1)\n",
    "    yb_squeezed = torch.squeeze(yb)\n",
    "    #print(xb.shape, yb.shape, preds.shape, yb_squeezed.shape)\n",
    "    correct = (preds[:,0]>0.5) == yb_squeezed\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_mnist = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_softmax = Learner(dls, nn.Linear(28*28,2), opt_func=SGD, loss_func=softmax_loss, metrics=batch_accuracy_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_mnist</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.637177</td>\n",
       "      <td>0.504163</td>\n",
       "      <td>0.494595</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.506652</td>\n",
       "      <td>0.218602</td>\n",
       "      <td>0.804453</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.187165</td>\n",
       "      <td>0.177797</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082407</td>\n",
       "      <td>0.109837</td>\n",
       "      <td>0.902065</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043772</td>\n",
       "      <td>0.081640</td>\n",
       "      <td>0.928929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.028667</td>\n",
       "      <td>0.065939</td>\n",
       "      <td>0.944256</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.055925</td>\n",
       "      <td>0.952323</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.048953</td>\n",
       "      <td>0.958454</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.043798</td>\n",
       "      <td>0.963779</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.039850</td>\n",
       "      <td>0.967570</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_mnist.fit(10, lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_softmax</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.393483</td>\n",
       "      <td>0.524685</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.089713</td>\n",
       "      <td>0.191804</td>\n",
       "      <td>0.841884</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.060771</td>\n",
       "      <td>0.111265</td>\n",
       "      <td>0.918361</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047979</td>\n",
       "      <td>0.080447</td>\n",
       "      <td>0.942965</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.041427</td>\n",
       "      <td>0.064905</td>\n",
       "      <td>0.954743</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.055631</td>\n",
       "      <td>0.962085</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035008</td>\n",
       "      <td>0.049473</td>\n",
       "      <td>0.966602</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>0.970152</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>0.041761</td>\n",
       "      <td>0.971442</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030383</td>\n",
       "      <td>0.039169</td>\n",
       "      <td>0.973459</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_softmax.fit(10, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a neural network instead of a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net_mnist = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net_softmax = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_mnist = Learner(dls, simple_net_mnist, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_softmax = Learner(dls, simple_net_softmax, opt_func=SGD, loss_func=softmax_loss, metrics=batch_accuracy_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_mnist</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.340772</td>\n",
       "      <td>0.400088</td>\n",
       "      <td>0.507180</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157275</td>\n",
       "      <td>0.240046</td>\n",
       "      <td>0.790094</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085858</td>\n",
       "      <td>0.122534</td>\n",
       "      <td>0.906986</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055504</td>\n",
       "      <td>0.082693</td>\n",
       "      <td>0.937883</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.041521</td>\n",
       "      <td>0.064401</td>\n",
       "      <td>0.950871</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034447</td>\n",
       "      <td>0.053921</td>\n",
       "      <td>0.959019</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.047091</td>\n",
       "      <td>0.964585</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>0.042260</td>\n",
       "      <td>0.967247</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>0.969587</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.035798</td>\n",
       "      <td>0.971926</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023454</td>\n",
       "      <td>0.033514</td>\n",
       "      <td>0.973136</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022505</td>\n",
       "      <td>0.031634</td>\n",
       "      <td>0.974669</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.976121</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021005</td>\n",
       "      <td>0.028717</td>\n",
       "      <td>0.976847</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.027564</td>\n",
       "      <td>0.978057</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019860</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.979348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.979348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>0.980074</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.980639</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_mnist.fit(20, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy_softmax</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.252010</td>\n",
       "      <td>0.427179</td>\n",
       "      <td>0.505486</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116402</td>\n",
       "      <td>0.219587</td>\n",
       "      <td>0.805744</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063833</td>\n",
       "      <td>0.114854</td>\n",
       "      <td>0.907228</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042337</td>\n",
       "      <td>0.079503</td>\n",
       "      <td>0.936673</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032580</td>\n",
       "      <td>0.062424</td>\n",
       "      <td>0.949903</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027662</td>\n",
       "      <td>0.052312</td>\n",
       "      <td>0.957325</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024847</td>\n",
       "      <td>0.045597</td>\n",
       "      <td>0.963537</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023005</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.966441</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021659</td>\n",
       "      <td>0.037145</td>\n",
       "      <td>0.970313</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.034299</td>\n",
       "      <td>0.972572</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019738</td>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.974024</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.019009</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.975395</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.976847</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.017836</td>\n",
       "      <td>0.027249</td>\n",
       "      <td>0.977735</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.026121</td>\n",
       "      <td>0.978380</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>0.978783</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016533</td>\n",
       "      <td>0.024297</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.023550</td>\n",
       "      <td>0.979590</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.015860</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>0.980155</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015565</td>\n",
       "      <td>0.022295</td>\n",
       "      <td>0.980800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_softmax.fit(20, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty cool accuracy numbers there! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
