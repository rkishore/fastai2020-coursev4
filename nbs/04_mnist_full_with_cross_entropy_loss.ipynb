{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy loss with MNIST_FULL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** In this notebook, I want to classify the full ten digits of the MNIST dataset (Q2 in \"Further Research\" section of Chapter 4 of the [textbook](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527). This is a key exercise to implement and understand as the lessons learned can then be extended to practically ANY classification application.\n",
    "\n",
    "Note that this notebook follows [this earlier notebook that classified two digits from the MNIST_SAMPLE dataset using the `cross_entropy` loss function](https://rkishore.github.io/2021/02/11/mnist_basics_with_cross_entropy_loss.html). In particular, the key additional learning was to define the `batch_accuracy` function for N columns of activations from the final layer of the model.\n",
    "\n",
    "Note that the original [MNIST dataset] (http://yann.lecun.com/exdb/mnist/) is a classic dataset of small (28x28 pixels), handwritten grayscale digits developed in the 1990s. The version we use here from fast.ai uses a standard PNG format instead of the special binary format of the original, making it easier to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the basic imports out of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/home/igolgi/.fastai/data/mnist_png/testing'),Path('/home/igolgi/.fastai/data/mnist_png/training')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerate and record the image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CATEGORIES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = [Path(str(path/'training') + '/%s/' % (i,)).ls().sorted() for i in range(MAX_CATEGORIES)]\n",
    "test_imgs = [Path(str(path/'testing') + '/%s/' % (i,)).ls().sorted() for i in range(MAX_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the images and convert them to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = [[tensor(Image.open(img)) for img in train_imgs[i]] for i in range(MAX_CATEGORIES)]\n",
    "test_tensors = [[tensor(Image.open(img)) for img in test_imgs[i]] for i in range(MAX_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(train_tensors[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEC0lEQVR4nO2bTU8kRRyHn6ru6ZnueQWGAZFFXOCwslE3EROTjXrRo56M8VN48Jtssp/Ci55N1EQvq0GMhw2oi0YWeRmYYWeYl56e7vJAcEOJeNGujtRz7D7UL8/8q+qfmi6hlMLyFGk6QNawQjSsEA0rRMMK0XCvevmWfO9/uwV9lnwsLntuK0TDCtGwQjSsEA0rRMMK0bhy200TGQSIwCd5bpbhbIC/20MetEiedEj6/fRypDbSPyDmn2Hwyk1+/ijH/fv32PzQp/XmIuLGXKo5MiMER5K4giAIWfV8vCAiyQlw0o2YHSGAUAqlBJGKMXVMkxkhUaNEeznHXKVjNEdmhBzd9mm8s8MHz35jNIfxXUa4LiKfJyrBQrHNpHNqNI9xIbJchqka4aRirfILC27baCzzU6Y+QX+lzng6YiW/T1VGRuMYr5Dh85Psv+by0tI2rxdGgG80j3EhYc0lvDFiqdQE4CgecBDnGB/5lHZHiG56XSpkQEhvRvL26kPeqGwC8DCq8qC3TGnbIb++SdwbpJrH/BoiwBFPu7BYSfqJhxyDGoaoOE41jvEK0YlwCBMXGSmS4TD18c1XSMawQjSMCRE5D1kuExeg5vYpyhCALzu3+HTrRYJmYiSXOSGFPLJWZRxAw+tQFCMAvtpfovBtkeLjdHeXc4wJkdUK0UKdcDpmzd+mKkPayZB2N8BvKuTpyEwuI6MCqlqiu1Ag3+jzal4x5SiasWT0JE9xb4Q8TbchO8cuqhpWiEZmhEggJxKQCuUIEJf+F51KjszgoEBwJsOQkMy07ttjj+8Gi+R3cwQ/7qFabSM5MiPk9/EE691F/EPBePtXYzmMTZnBfJmjO4JbM/umIlyKMSHhhIu31OGFypmQWGVjOUt9yjhTkzBT52RF8v7yBndLWwB0kwLNYQkZmf2KK3UhIggYzpYZzsS8W9mg7kSATy/JcxL6iHHaiS6Sep2qoMCwnoNKxLw7pibPfpNP9l6m+cUctUdh2pEukHqFqJxL5Au8QkRVFgBISNg5rjG9GeMddEn30PAixrfd38YDfoqmGB0EFHd6iJOu0TzGl/bjJM+jUQO3K5EnPdTwmk0ZnVZcYqs/i9cRcHhMEpoVYrxCRsphEHuI8xPD5Jptuzpr+UMWGp+zfneex/FtZh70kV9/byyP8QqpOz6rnsudxi6nN2PCKc9oHuNCzomURETi6dQxRPqdahzjRIqw57ExSs7OQICd0wmcvkCOzBoRV11C/C+uh8ggQFbKJI0JhnPlP597JyFOqwfHbeLj1r897F/4u+shqVdI0u+ffYi7f4D3w8V3JjvUczKzhmQFK0TDCtGwQjSsEA0rROPKPuQ6YitEwwrRsEI0rBANK0TDCtH4A72eR0xU8wdLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(train_tensors[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(train_tensors[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5923, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_train_tensors = torch.stack(train_tensors[0]).float()/255.\n",
    "stacked_train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_train_tensors = [torch.stack(train_tensors[i]).float()/255. for i in range(MAX_CATEGORIES)]\n",
    "stacked_test_tensors = [torch.stack(test_tensors[i]).float()/255. for i in range(MAX_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5923, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_train_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat(stacked_train_tensors); train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the training data, there are a total of 60000 images of 28x28 pixels each for the digits from 0 to 9. Let's \"flatten\" the 28x28 matrices corresponding to each image into one long row of 784 pixel values per image. This representation is easier to deal with as we will see later. Basically, we can then come up with a weight per pixel that has to be learnt in order to classify the ten digits accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.view(-1, 28*28); train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.cat(stacked_test_tensors); test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, for the validation dataset, there are 10000 images of 28x28 pixels each for the digits from 0 to 9. Let's \"flatten\" the 28x28 matrices corresponding to each image into one long row of 784 pixel values per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = test_x.view(-1, 28*28); test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the labels for each of these images. Basically, the label for each digit is simply the integer corresponding to that digit. So, all 0 images have a 0 label, all 1 images have a 1 label and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_list = []\n",
    "for i in range(MAX_CATEGORIES):\n",
    "    train_y_list += [i]*len(train_imgs[i])\n",
    "len(train_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor(train_y_list); train_y.shape\n",
    "#train_y = tensor([1]*len(train_imgs[0]) + [0]*len(train_imgs[1])); train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are 60000 labels, one for each training image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the exact same process to label the validation/test images. Note that for both the training and validation label tensor, we don't use `torch.unsqueeze` as in the [earlier notebook](https://rkishore.github.io/2021/02/11/mnist_basics_with_cross_entropy_loss.html) as we don't need it here. The labels `y` being represented by a rank-1 tensor (a vector) works just fine. In fact, you would have noticed in the [earlier notebook](https://rkishore.github.io/2021/02/11/mnist_basics_with_cross_entropy_loss.html) that we have to use `torch.squeeze` in the `batch_accuracy` function so as to get a rank-1 tensor (a vector) from a rank-2 tensor (a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_list = []\n",
    "for i in range(MAX_CATEGORIES):\n",
    "    test_y_list += [i]*len(test_imgs[i])\n",
    "#test_y = tensor(test_y_list).unsqueeze(1); test_y.shape\n",
    "test_y = tensor(test_y_list); test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are 10000 labels, one for each validation image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = list(zip(train_x, train_y))\n",
    "valid_dset = list(zip(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dset, batch_size=256, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross-entropy loss function: we take the log_softmax of the activations returned by the model, and \n",
    "# then use the nll_loss function to select the loss corresponding to the target index\n",
    "def ce_loss(preds, tgts):\n",
    "    preds = torch.log_softmax(preds, dim=1)\n",
    "    return F.nll_loss(preds, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model, loss_func):\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params, loss_func):\n",
    "    acc_loss = []\n",
    "    for xb, yb in dl:\n",
    "        acc_loss.append(calc_grad(xb, yb, model, loss_func))\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "    print(\"Mean loss: \", tensor(acc_loss).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb,yb):\n",
    "    _, preds = torch.max(xb, 1)\n",
    "    correct = (preds == yb)\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb),yb) for xb,yb in valid_dl]\n",
    "    #print(accs)\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 9, 8, 9, 5, 7, 1, 8, 6, 9, 6, 8, 3, 5, 4, 7, 7, 7, 1, 1, 0, 9, 3, 3, 4, 7, 2, 1, 6, 5, 3, 1, 2, 4, 8, 8, 8, 7, 7, 2, 8, 8, 8, 6, 5, 4, 8, 2, 7, 7, 9, 6, 7, 8, 6, 0, 1, 6, 9, 9, 7, 9, 5, 1,\n",
       "        6, 2, 3, 2, 8, 2, 8, 4, 1, 4, 8, 6, 7, 9, 1, 9, 8, 1, 2, 7, 2, 0, 6, 2, 8, 7, 8, 2, 9, 4, 6, 2, 6, 3, 4, 8, 6, 1, 5, 3, 1, 3, 7, 2, 0, 0, 1, 6, 2, 6, 1, 0, 4, 1, 1, 5, 7, 3, 9, 7, 0, 3, 8, 9,\n",
       "        8, 1, 6, 6, 4, 5, 1, 8, 0, 5, 6, 5, 9, 5, 7, 5, 6, 0, 1, 8, 1, 2, 8, 9, 9, 4, 7, 4, 8, 3, 6, 4, 5, 6, 8, 5, 3, 6, 4, 3, 4, 6, 1, 5, 5, 1, 8, 7, 8, 5, 9, 6, 1, 7, 4, 7, 2, 7, 3, 1, 0, 1, 7, 2,\n",
       "        9, 3, 4, 4, 5, 2, 5, 9, 1, 6, 7, 0, 3, 3, 0, 5, 7, 0, 9, 3, 5, 2, 0, 1, 2, 6, 6, 2, 0, 4, 5, 5, 3, 8, 0, 7, 6, 2, 0, 7, 5, 1, 6, 7, 5, 2, 8, 9, 2, 6, 4, 3, 2, 6, 5, 0, 3, 0, 3, 7, 1, 6, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2932, -0.3405,  0.3322,  0.1191, -0.0160, -0.2307,  0.3889, -0.0661,  0.3204, -0.0289],\n",
       "        [-0.1252, -0.1254,  0.2319,  0.0717, -0.2982, -0.1123,  0.1948, -0.1071,  0.1214, -0.1901],\n",
       "        [-0.4835, -0.3397,  0.2349,  0.1295, -0.3830, -0.2500,  0.6015, -0.4489,  0.2581,  0.0169],\n",
       "        [-0.0691,  0.0671,  0.2555,  0.0100, -0.2683, -0.1499,  0.4475,  0.0905, -0.2517, -0.1111]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28, MAX_CATEGORIES)\n",
    "batch = train_x[:4]\n",
    "preds = linear_model(batch); preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2436, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ce_loss(preds, y[:4]); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(preds, train_y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss:  tensor(1.5714)\n",
      "Accuracy:  0.8233\n",
      "Mean loss:  tensor(0.9475)\n",
      "Accuracy:  0.8448\n",
      "Mean loss:  tensor(0.7535)\n",
      "Accuracy:  0.8598\n",
      "Mean loss:  tensor(0.6580)\n",
      "Accuracy:  0.8696\n",
      "Mean loss:  tensor(0.6007)\n",
      "Accuracy:  0.8726\n",
      "Mean loss:  tensor(0.5605)\n",
      "Accuracy:  0.8787\n",
      "Mean loss:  tensor(0.5316)\n",
      "Accuracy:  0.8826\n",
      "Mean loss:  tensor(0.5090)\n",
      "Accuracy:  0.882\n",
      "Mean loss:  tensor(0.4909)\n",
      "Accuracy:  0.8832\n",
      "Mean loss:  tensor(0.4762)\n",
      "Accuracy:  0.8845\n",
      "Mean loss:  tensor(0.4636)\n",
      "Accuracy:  0.8846\n",
      "Mean loss:  tensor(0.4533)\n",
      "Accuracy:  0.89\n",
      "Mean loss:  tensor(0.4436)\n",
      "Accuracy:  0.8922\n",
      "Mean loss:  tensor(0.4355)\n",
      "Accuracy:  0.8929\n",
      "Mean loss:  tensor(0.4281)\n",
      "Accuracy:  0.892\n",
      "Mean loss:  tensor(0.4220)\n",
      "Accuracy:  0.8929\n",
      "Mean loss:  tensor(0.4156)\n",
      "Accuracy:  0.8983\n",
      "Mean loss:  tensor(0.4107)\n",
      "Accuracy:  0.8948\n",
      "Mean loss:  tensor(0.4053)\n",
      "Accuracy:  0.9013\n",
      "Mean loss:  tensor(0.4009)\n",
      "Accuracy:  0.9021\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28, MAX_CATEGORIES)\n",
    "w,b = linear_model.parameters()\n",
    "params = w,b\n",
    "lr = 1e-2\n",
    "for i in range(20):\n",
    "    train_epoch(linear_model, lr, params, ce_loss)\n",
    "    print(\"Accuracy: \", validate_epoch(linear_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see reducing training loss and increasing accuracy. This is what we want to achieve.\n",
    "\n",
    "And, with a simple linear classifier, we get an accuracy of 90.21%! Note that the [original MNIST work from the 1990s](http://yann.lecun.com/exdb/mnist/) reports error rates of 8.4 and 12.0% so we are in the ballpark. We do use 3-channel RGB images here as compared to 1-channel grayscale images they used so this may have something to do with the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's replace our code with PyTorch/fastai built-in functions and see if we get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28, MAX_CATEGORIES) # built-in linear classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(linear_model.parameters(), lr) # built-in SGD optimizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine the `train_epoch` function to use the optimizer function `opt` defined above (which uses SGD inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    acc_loss = []\n",
    "    for xb, yb in dl:\n",
    "        acc_loss.append(calc_grad(xb, yb, model, ce_loss))\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(\"Mean loss: \", tensor(acc_loss).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a `train_model` function to make our life easier (as shown in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(\"Accuracy: \", validate_epoch(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss:  tensor(1.5554)\n",
      "Accuracy:  0.8134\n",
      "Mean loss:  tensor(0.9509)\n",
      "Accuracy:  0.8435\n",
      "Mean loss:  tensor(0.7575)\n",
      "Accuracy:  0.862\n",
      "Mean loss:  tensor(0.6616)\n",
      "Accuracy:  0.8702\n",
      "Mean loss:  tensor(0.6031)\n",
      "Accuracy:  0.8741\n",
      "Mean loss:  tensor(0.5628)\n",
      "Accuracy:  0.8771\n",
      "Mean loss:  tensor(0.5339)\n",
      "Accuracy:  0.8796\n",
      "Mean loss:  tensor(0.5107)\n",
      "Accuracy:  0.8847\n",
      "Mean loss:  tensor(0.4928)\n",
      "Accuracy:  0.8869\n",
      "Mean loss:  tensor(0.4778)\n",
      "Accuracy:  0.8887\n",
      "Mean loss:  tensor(0.4650)\n",
      "Accuracy:  0.889\n",
      "Mean loss:  tensor(0.4542)\n",
      "Accuracy:  0.8889\n",
      "Mean loss:  tensor(0.4447)\n",
      "Accuracy:  0.8905\n",
      "Mean loss:  tensor(0.4369)\n",
      "Accuracy:  0.8931\n",
      "Mean loss:  tensor(0.4293)\n",
      "Accuracy:  0.8951\n",
      "Mean loss:  tensor(0.4230)\n",
      "Accuracy:  0.8981\n",
      "Mean loss:  tensor(0.4166)\n",
      "Accuracy:  0.896\n",
      "Mean loss:  tensor(0.4115)\n",
      "Accuracy:  0.8972\n",
      "Mean loss:  tensor(0.4067)\n",
      "Accuracy:  0.8964\n",
      "Mean loss:  tensor(0.4018)\n",
      "Accuracy:  0.8943\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28, MAX_CATEGORIES)\n",
    "lr = 1e-2\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a neural network instead of a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple neural network with 3 layers. The first layer is a linear classifier (like the one we saw so far) that aims to capture 30 \"features\". This is followed by a non-linearity (ReLU), which is followed by another linear layer that outputs MAX_CATEGORIES (ten) activations, one per category. We will use this simple neural network instead of the linear classifier, keeping all else the same. \n",
    "\n",
    "In other words, we use the same loss function, the same accuracy function, the same SGD optimizer but just change the underlying \"network architecture\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,MAX_CATEGORIES)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss:  tensor(2.0894)\n",
      "Accuracy:  0.6733\n",
      "Mean loss:  tensor(1.4704)\n",
      "Accuracy:  0.7968\n",
      "Mean loss:  tensor(0.9518)\n",
      "Accuracy:  0.846\n",
      "Mean loss:  tensor(0.7077)\n",
      "Accuracy:  0.861\n",
      "Mean loss:  tensor(0.5910)\n",
      "Accuracy:  0.8754\n",
      "Mean loss:  tensor(0.5248)\n",
      "Accuracy:  0.8792\n",
      "Mean loss:  tensor(0.4814)\n",
      "Accuracy:  0.8885\n",
      "Mean loss:  tensor(0.4511)\n",
      "Accuracy:  0.891\n",
      "Mean loss:  tensor(0.4285)\n",
      "Accuracy:  0.8944\n",
      "Mean loss:  tensor(0.4110)\n",
      "Accuracy:  0.8985\n",
      "Mean loss:  tensor(0.3965)\n",
      "Accuracy:  0.8976\n",
      "Mean loss:  tensor(0.3848)\n",
      "Accuracy:  0.8984\n",
      "Mean loss:  tensor(0.3752)\n",
      "Accuracy:  0.8989\n",
      "Mean loss:  tensor(0.3664)\n",
      "Accuracy:  0.9026\n",
      "Mean loss:  tensor(0.3588)\n",
      "Accuracy:  0.9047\n",
      "Mean loss:  tensor(0.3523)\n",
      "Accuracy:  0.9075\n",
      "Mean loss:  tensor(0.3464)\n",
      "Accuracy:  0.899\n",
      "Mean loss:  tensor(0.3407)\n",
      "Accuracy:  0.9087\n",
      "Mean loss:  tensor(0.3357)\n",
      "Accuracy:  0.9073\n",
      "Mean loss:  tensor(0.3315)\n",
      "Accuracy:  0.9105\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "opt = SGD(simple_net.parameters(), lr)\n",
    "train_model(simple_net, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Our code above can classify two-ten MNIST digits (just change the MAX_CATEGORIES value and re-run the notebook). Yay! We have looked at using a linear classifier as well as a simple neural net with 2 linear layers and 1 ReLU in between them. After 20 epochs, we get ~90% accuracy for both \"network architectures\". We use our cross-entropy loss function `ce_loss`, the `batch_accuracy` functions as well as stochastic gradient descent (SGD) as our optimizer function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try the built-in Learner class now with our loss and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define simple_net so that we start training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,MAX_CATEGORIES)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD, loss_func=ce_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.983459</td>\n",
       "      <td>1.864117</td>\n",
       "      <td>0.558300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.362842</td>\n",
       "      <td>1.218018</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.824246</td>\n",
       "      <td>0.828200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.698222</td>\n",
       "      <td>0.642191</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.584295</td>\n",
       "      <td>0.545529</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.520027</td>\n",
       "      <td>0.487446</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.474387</td>\n",
       "      <td>0.448316</td>\n",
       "      <td>0.885400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.443602</td>\n",
       "      <td>0.420808</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.427964</td>\n",
       "      <td>0.399631</td>\n",
       "      <td>0.893100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.404308</td>\n",
       "      <td>0.383699</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.394066</td>\n",
       "      <td>0.370683</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.383982</td>\n",
       "      <td>0.359945</td>\n",
       "      <td>0.899700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.371369</td>\n",
       "      <td>0.351005</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.365749</td>\n",
       "      <td>0.342824</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.356426</td>\n",
       "      <td>0.335881</td>\n",
       "      <td>0.904400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.344545</td>\n",
       "      <td>0.329886</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.335118</td>\n",
       "      <td>0.324293</td>\n",
       "      <td>0.907700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.338555</td>\n",
       "      <td>0.319021</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.331340</td>\n",
       "      <td>0.315188</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.310206</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(20, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get >91% batch_accuracy with the built-in Learner function using our loss function `ce_loss` and our accuracy function `batch_accuracy`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the built-in cnn_learner function from fastai next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the built-in fastai library functions, we have to use the higher-level library functions to read in the data as tensors, split it into training and validation dataloaders, and perform any data augmentations to improve our accuracy. Now, we are starting to use the state-of-the-art instead of our own hand-written functions or their equivalents. We use the fastai `DataBlock` API to do all of the above. \n",
    "\n",
    "Note that the lines:\n",
    "    \n",
    "    item_tfms=Resize(32),\n",
    "    batch_tfms=aug_transforms(size=28, min_scale=0.75)\n",
    "     \n",
    "implement a fastai data augmentation strategy called **presizing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                  get_items=get_image_files,\n",
    "                  splitter=GrandparentSplitter(train_name=('training'), valid_name='testing'),\n",
    "                  get_y=parent_label,\n",
    "                  item_tfms=Resize(32),\n",
    "                  batch_tfms=aug_transforms(size=28, min_scale=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = mnist.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#60000) [(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9))...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataset shows 60000 tensors corresponding to the 60000 images as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10000) [(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9)),(PILImage mode=RGB size=28x28, TensorCategory(9))...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation dataset shows 10000 tensors corresponding to the 10000 images as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorCategory([1, 3, 9, 2, 1, 3, 9, 6, 6, 4, 4, 3, 6, 2, 9, 6, 4, 3, 7, 6, 3, 6, 1, 7, 4, 6, 8, 4, 8, 3, 0, 2, 9, 4, 1, 6, 6, 2, 8, 0, 5, 6, 4, 2, 7, 3, 2, 6, 4, 2, 5, 1, 2, 1, 6, 3, 8, 9, 3, 9, 1, 4, 0, 8],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our default batch size is 64 so the `y` (dependent variable or labels) tensor shows 64 rows. Each row is a single integer between 0 and 9, representing the ten possible digits. \n",
    "\n",
    "Let's use the built-in `show_batch` function to look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(nrows=1, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the cnn_learner with the resnet18 network, cross-entropy loss, and the built-in accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the built-in [learning rate finder](https://arxiv.org/abs/1506.01186) to find a good learning rate to use while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_min, lr_steep = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum/10: 5.75e-03, steepest point: 2.09e-03\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the plot above that the loss does not start decreasing till around 1e-4. Nothing really happens from 1e-7 to 1e-4. Beyond 1e-4, it decreases till around 1e-1, beyond which it starts increasing. We don't want a learning rate greater than 1e-1.\n",
    "\n",
    "The number `lr_min` tells us the order-of-magnitude lower learning rate corresponding to the learning rate where the minimum loss was achieved. The number `lr_steep` tells us the last learning rate where the loss was clearly decreasing. The textbook asks us to pick one of these values or a learning rate around these values as a rule-of-thumb to pick a good learning rate. We pick lr=3e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.419033</td>\n",
       "      <td>0.938879</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.522387</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.182286</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226973</td>\n",
       "      <td>0.145478</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171017</td>\n",
       "      <td>0.097725</td>\n",
       "      <td>0.969800</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.154143</td>\n",
       "      <td>0.082914</td>\n",
       "      <td>0.973600</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(5, base_lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Just like that, we are at 97.3% accuracy. Let's save this model for future use/training/refining. Note that the world record for MNIST right now is 99.75% accuracy (https://www.kaggle.com/c/digit-recognizer/discussion/61480). We can try to achieve higher accuracy by training for longer and also using discriminative learning rates. More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/mnist-full-resnet18-best.pth')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save(\"mnist-full-resnet18-best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "Let's now explore fastai's built-in functions to interpret how our model does. Note that the textbook says that loss functions are very hard to interpret as they designed for computers to differentiate and optimize, and not something that people can understand. This is what metrics (such as accuracy are for). In this case, our accuracy is looking pretty good already but where are we making mistakes? \n",
    "\n",
    "We can use a `confusion matrix` to see where our model is doing well and where its doing badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "==:\n100000\n10000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-1bfbda956130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/interpret.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(self, normalize, title, cmap, norm_dec, plot_txt, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;34m\"Plot the confusion matrix, with `title` and using `cmap`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# This function is mainly copied from the sklearn docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/interpret.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m\"Confusion matrix as an `np.ndarray`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mflatten_check\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;34m\"Check that `out` and `targ` have the same number of elements and flatten them.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest_eq\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"`test` that `a==b`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'=='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ==:\n100000\n10000"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "==:\n100000\n10000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-cb09b158d58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_confused\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/interpret.py\u001b[0m in \u001b[0;36mmost_confused\u001b[0;34m(self, min_val)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_confused\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;34m\"Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         res = [(self.vocab[i],self.vocab[j],cm[i,j])\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/interpret.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m\"Confusion matrix as an `np.ndarray`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mflatten_check\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;34m\"Check that `out` and `targ` have the same number of elements and flatten them.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest_eq\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"`test` that `a==b`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'=='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bigdrive1/cnn/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ==:\n100000\n10000"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "interp.most_confused(min_val=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ok, back to improving the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
